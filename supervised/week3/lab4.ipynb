{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/afifaf20/courserasupervisedweek3lab4?scriptVersionId=198318339\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom plt_logistic_loss import  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example\nfrom plt_logistic_loss import soup_bowl, plt_logistic_squared_error\nplt.style.use('./deeplearning.mplstyle')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-26T04:39:31.678643Z","iopub.execute_input":"2024-09-26T04:39:31.679175Z","iopub.status.idle":"2024-09-26T04:39:31.716782Z","shell.execute_reply.started":"2024-09-26T04:39:31.679126Z","shell.execute_reply":"2024-09-26T04:39:31.714889Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidget\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplt_logistic_loss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplt_logistic_loss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m soup_bowl, plt_logistic_squared_error\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./deeplearning.mplstyle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plt_logistic_loss'"],"ename":"ModuleNotFoundError","evalue":"No module named 'plt_logistic_loss'","output_type":"error"}]},{"cell_type":"markdown","source":"## Squared error for logistic regression?\n<img align=\"left\" src=\"./images/C1_W3_SqErrorVsLogistic.png\"     style=\" width:400px; padding: 10px; \" > Recall for **Linear** Regression we have used the **squared error cost function**:\nThe equation for the squared error cost with one variable is:\n  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$ \n \nwhere \n  $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$\n","metadata":{}},{"cell_type":"markdown","source":"Recall, the squared error cost had the nice property that following the derivative of the cost leads to the minimum.","metadata":{}},{"cell_type":"code","source":"soup_bowl()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T04:39:32.600917Z","iopub.execute_input":"2024-09-26T04:39:32.601555Z","iopub.status.idle":"2024-09-26T04:39:32.639558Z","shell.execute_reply.started":"2024-09-26T04:39:32.601497Z","shell.execute_reply":"2024-09-26T04:39:32.637226Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msoup_bowl\u001b[49m()\n","\u001b[0;31mNameError\u001b[0m: name 'soup_bowl' is not defined"],"ename":"NameError","evalue":"name 'soup_bowl' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"This cost function worked well for linear regression, it is natural to consider it for logistic regression as well. However, as the slide above points out, $f_{wb}(x)$ now has a non-linear component, the sigmoid function:   $f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$.   Let's try a squared error cost on the example from an earlier lab, now including the sigmoid.\n\nHere is our training data:","metadata":{}},{"cell_type":"code","source":"x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)\ny_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)\nplt_simple_example(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T04:39:33.679392Z","iopub.execute_input":"2024-09-26T04:39:33.679881Z","iopub.status.idle":"2024-09-26T04:39:33.717064Z","shell.execute_reply.started":"2024-09-26T04:39:33.679835Z","shell.execute_reply":"2024-09-26T04:39:33.714965Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m],dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlongdouble)\n\u001b[1;32m      2\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m,  \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlongdouble)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt_simple_example\u001b[49m(x_train, y_train)\n","\u001b[0;31mNameError\u001b[0m: name 'plt_simple_example' is not defined"],"ename":"NameError","evalue":"name 'plt_simple_example' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"Now, let's get a surface plot of the cost using a *squared error cost*:\n  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$ \n \nwhere \n  $$f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$$\n","metadata":{}},{"cell_type":"code","source":"plt.close('all')\nplt_logistic_squared_error(x_train,y_train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T04:39:34.487358Z","iopub.execute_input":"2024-09-26T04:39:34.487893Z","iopub.status.idle":"2024-09-26T04:39:34.593123Z","shell.execute_reply.started":"2024-09-26T04:39:34.487847Z","shell.execute_reply":"2024-09-26T04:39:34.591457Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt_logistic_squared_error\u001b[49m(x_train,y_train)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n","\u001b[0;31mNameError\u001b[0m: name 'plt_logistic_squared_error' is not defined"],"ename":"NameError","evalue":"name 'plt_logistic_squared_error' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number. \n\n>**Definition Note:**   In this course, these definitions are used:  \n**Loss** is a measure of the difference of a single example to its target value while the  \n**Cost** is a measure of the losses over the training set\n\n\nThis is defined: \n* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n\n\\begin{equation}\n  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n  \\end{cases}\n\\end{equation}\n\n\n*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.\n\n*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ where function $g$ is the sigmoid function.\n\nThe defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:","metadata":{}},{"cell_type":"code","source":"plt_two_logistic_loss_curves()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T04:39:35.446432Z","iopub.execute_input":"2024-09-26T04:39:35.446901Z","iopub.status.idle":"2024-09-26T04:39:35.481948Z","shell.execute_reply.started":"2024-09-26T04:39:35.446855Z","shell.execute_reply":"2024-09-26T04:39:35.479934Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt_two_logistic_loss_curves\u001b[49m()\n","\u001b[0;31mNameError\u001b[0m: name 'plt_two_logistic_loss_curves' is not defined"],"ename":"NameError","evalue":"name 'plt_two_logistic_loss_curves' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"The loss function above can be rewritten to be easier to implement.\n    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n  \nThis is a rather formidable-looking equation. It is less daunting when you consider $y^{(i)}$ can have only two values, 0 and 1. One can then consider the equation in two pieces:  \nwhen $ y^{(i)} = 0$, the left-hand term is eliminated:\n$$\n\\begin{align}\nloss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n\\end{align}\n$$\nand when $ y^{(i)} = 1$, the right-hand term is eliminated:\n$$\n\\begin{align}\n  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n  &=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n\\end{align}\n$$\n\nOK, with this new logistic loss function, a cost function can be produced that incorporates the loss from all the examples. This will be the topic of the next lab. For now, let's take a look at the cost vs parameters curve for the simple example we considered above:","metadata":{}},{"cell_type":"code","source":"plt.close('all')\ncst = plt_logistic_cost(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T04:39:36.895754Z","iopub.execute_input":"2024-09-26T04:39:36.896223Z","iopub.status.idle":"2024-09-26T04:39:36.930344Z","shell.execute_reply.started":"2024-09-26T04:39:36.89618Z","shell.execute_reply":"2024-09-26T04:39:36.928198Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m cst \u001b[38;5;241m=\u001b[39m \u001b[43mplt_logistic_cost\u001b[49m(x_train,y_train)\n","\u001b[0;31mNameError\u001b[0m: name 'plt_logistic_cost' is not defined"],"ename":"NameError","evalue":"name 'plt_logistic_cost' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}