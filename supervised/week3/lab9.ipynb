{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/afifaf20/courserasupervisedweek3lab9?scriptVersionId=198320727\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom plt_overfit import overfit_example, output\nfrom lab_utils_common import sigmoid\nnp.set_printoptions(precision=8)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-26T05:06:08.05061Z","iopub.execute_input":"2024-09-26T05:06:08.051062Z","iopub.status.idle":"2024-09-26T05:06:08.0858Z","shell.execute_reply.started":"2024-09-26T05:06:08.05102Z","shell.execute_reply":"2024-09-26T05:06:08.084055Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidget\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplt_overfit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m overfit_example, output\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlab_utils_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sigmoid\n\u001b[1;32m      6\u001b[0m np\u001b[38;5;241m.\u001b[39mset_printoptions(precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plt_overfit'"],"ename":"ModuleNotFoundError","evalue":"No module named 'plt_overfit'","output_type":"error"}]},{"cell_type":"markdown","source":"## Cost functions with regularization\n### Cost function for regularized linear regression\n\nThe equation for the cost function regularized linear regression is:\n$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{1}$$ \nwhere:\n$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{2} $$ \n\n\nCompare this to the cost function without regularization (which you implemented in  a previous lab), which is of the form:\n\n$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$ \n\nThe difference is the regularization term,  <span style=\"color:blue\">\n    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span> \n    \nIncluding this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter $b$ is not regularized. This is standard practice.\n\nBelow is an implementation of equations (1) and (2). Note that this uses a *standard pattern for this course*,   a `for loop` over all `m` examples.","metadata":{}},{"cell_type":"code","source":"def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n    \"\"\"\n    Computes the cost over all examples\n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n    Returns:\n      total_cost (scalar):  cost \n    \"\"\"\n\n    m  = X.shape[0]\n    n  = len(w)\n    cost = 0.\n    for i in range(m):\n        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot\n        cost = cost + (f_wb_i - y[i])**2                               #scalar             \n    cost = cost / (2 * m)                                              #scalar  \n \n    reg_cost = 0\n    for j in range(n):\n        reg_cost += (w[j]**2)                                          #scalar\n    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n    \n    total_cost = cost + reg_cost                                       #scalar\n    return total_cost   ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T05:06:08.600934Z","iopub.execute_input":"2024-09-26T05:06:08.601383Z","iopub.status.idle":"2024-09-26T05:06:08.611317Z","shell.execute_reply.started":"2024-09-26T05:06:08.60134Z","shell.execute_reply":"2024-09-26T05:06:08.609992Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1)\nX_tmp = np.random.rand(5,6)\ny_tmp = np.array([0,1,0,1,0])\nw_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\nb_tmp = 0.5\nlambda_tmp = 0.7\ncost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n\nprint(\"Regularized cost:\", cost_tmp)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T05:06:08.863527Z","iopub.execute_input":"2024-09-26T05:06:08.86396Z","iopub.status.idle":"2024-09-26T05:06:08.872953Z","shell.execute_reply.started":"2024-09-26T05:06:08.863918Z","shell.execute_reply":"2024-09-26T05:06:08.871778Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Regularized cost: 0.07917239320214277\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Cost function for regularized logistic regression\nFor regularized **logistic** regression, the cost function is of the form\n$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{3}$$\nwhere:\n$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = sigmoid(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)  \\tag{4} $$ \n\nCompare this to the cost function without regularization (which you implemented in  a previous lab):\n\n$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right] $$\n\nAs was the case in linear regression above, the difference is the regularization term, which is    <span style=\"color:blue\">\n    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span> \n\nIncluding this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter $b$ is not regularized. This is standard practice. ","metadata":{}},{"cell_type":"code","source":"def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n    \"\"\"\n    Computes the cost over all examples\n    Args:\n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n    Returns:\n      total_cost (scalar):  cost \n    \"\"\"\n\n    m,n  = X.shape\n    cost = 0.\n    for i in range(m):\n        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot\n        f_wb_i = sigmoid(z_i)                                          #scalar\n        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n             \n    cost = cost/m                                                      #scalar\n\n    reg_cost = 0\n    for j in range(n):\n        reg_cost += (w[j]**2)                                          #scalar\n    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n    \n    total_cost = cost + reg_cost                                       #scalar\n    return total_cost          ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T05:06:09.440247Z","iopub.execute_input":"2024-09-26T05:06:09.440695Z","iopub.status.idle":"2024-09-26T05:06:09.450832Z","shell.execute_reply.started":"2024-09-26T05:06:09.440651Z","shell.execute_reply":"2024-09-26T05:06:09.449536Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1)\nX_tmp = np.random.rand(5,6)\ny_tmp = np.array([0,1,0,1,0])\nw_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\nb_tmp = 0.5\nlambda_tmp = 0.7\ncost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n\nprint(\"Regularized cost:\", cost_tmp)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T05:06:09.737881Z","iopub.execute_input":"2024-09-26T05:06:09.738343Z","iopub.status.idle":"2024-09-26T05:06:09.802283Z","shell.execute_reply.started":"2024-09-26T05:06:09.7383Z","shell.execute_reply":"2024-09-26T05:06:09.800295Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m b_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      6\u001b[0m lambda_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n\u001b[0;32m----> 7\u001b[0m cost_tmp \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cost_logistic_reg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_tmp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegularized cost:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cost_tmp)\n","Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mcompute_cost_logistic_reg\u001b[0;34m(X, y, w, b, lambda_)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m):\n\u001b[1;32m     18\u001b[0m     z_i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X[i], w) \u001b[38;5;241m+\u001b[39m b                                      \u001b[38;5;66;03m#(n,)(n,)=scalar, see np.dot\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     f_wb_i \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid\u001b[49m(z_i)                                          \u001b[38;5;66;03m#scalar\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m-\u001b[39my[i]\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(f_wb_i) \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my[i])\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mf_wb_i)      \u001b[38;5;66;03m#scalar\u001b[39;00m\n\u001b[1;32m     22\u001b[0m cost \u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m/\u001b[39mm                                                      \u001b[38;5;66;03m#scalar\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'sigmoid' is not defined"],"ename":"NameError","evalue":"name 'sigmoid' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"## Gradient descent with regularization\nThe basic algorithm for running gradient descent does not change with regularization, it is:\n$$\\begin{align*}\n&\\text{repeat until convergence:} \\; \\lbrace \\\\\n&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\ \n&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n&\\rbrace\n\\end{align*}$$\nWhere each iteration performs simultaneous updates on $w_j$ for all $j$.\n\nWhat changes with regularization is computing the gradients.","metadata":{}},{"cell_type":"markdown","source":"### Computing the Gradient with regularization (both linear/logistic)\nThe gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\\mathbf{w}b}$.\n$$\\begin{align*}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{2} \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n\\end{align*}$$\n\n* m is the number of training examples in the data set      \n* $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target\n\n      \n* For a  <span style=\"color:blue\"> **linear** </span> regression model  \n    $f_{\\mathbf{w},b}(x) = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n* For a <span style=\"color:blue\"> **logistic** </span> regression model  \n    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n    $f_{\\mathbf{w},b}(x) = g(z)$  \n    where $g(z)$ is the sigmoid function:  \n    $g(z) = \\frac{1}{1+e^{-z}}$   \n    \nThe term which adds regularization is  the <span style=\"color:blue\">$\\frac{\\lambda}{m} w_j $</span>.","metadata":{}},{"cell_type":"code","source":"def compute_gradient_linear_reg(X, y, w, b, lambda_): \n    \"\"\"\n    Computes the gradient for linear regression \n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n      \n    Returns:\n      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m,n = X.shape           #(number of examples, number of features)\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n\n    for i in range(m):                             \n        err = (np.dot(X[i], w) + b) - y[i]                 \n        for j in range(n):                         \n            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n        dj_db = dj_db + err                        \n    dj_dw = dj_dw / m                                \n    dj_db = dj_db / m   \n    \n    for j in range(n):\n        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n\n    return dj_db, dj_dw","metadata":{"execution":{"iopub.status.busy":"2024-09-26T05:06:10.567921Z","iopub.execute_input":"2024-09-26T05:06:10.569084Z","iopub.status.idle":"2024-09-26T05:06:10.578295Z","shell.execute_reply.started":"2024-09-26T05:06:10.569035Z","shell.execute_reply":"2024-09-26T05:06:10.57685Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1)\nX_tmp = np.random.rand(5,3)\ny_tmp = np.array([0,1,0,1,0])\nw_tmp = np.random.rand(X_tmp.shape[1])\nb_tmp = 0.5\nlambda_tmp = 0.7\ndj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n\nprint(f\"dj_db: {dj_db_tmp}\", )\nprint(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )","metadata":{"execution":{"iopub.status.busy":"2024-09-26T05:06:10.839964Z","iopub.execute_input":"2024-09-26T05:06:10.840417Z","iopub.status.idle":"2024-09-26T05:06:10.849294Z","shell.execute_reply.started":"2024-09-26T05:06:10.840369Z","shell.execute_reply":"2024-09-26T05:06:10.848056Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"dj_db: 0.6648774569425726\nRegularized dj_dw:\n [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n    \"\"\"\n    Computes the gradient for linear regression \n \n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n    Returns\n      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m,n = X.shape\n    dj_dw = np.zeros((n,))                            #(n,)\n    dj_db = 0.0                                       #scalar\n\n    for i in range(m):\n        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n        err_i  = f_wb_i  - y[i]                       #scalar\n        for j in range(n):\n            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n        dj_db = dj_db + err_i\n    dj_dw = dj_dw/m                                   #(n,)\n    dj_db = dj_db/m                                   #scalar\n\n    for j in range(n):\n        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n\n    return dj_db, dj_dw  ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T05:06:11.152149Z","iopub.execute_input":"2024-09-26T05:06:11.152587Z","iopub.status.idle":"2024-09-26T05:06:11.162404Z","shell.execute_reply.started":"2024-09-26T05:06:11.152545Z","shell.execute_reply":"2024-09-26T05:06:11.161137Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1)\nX_tmp = np.random.rand(5,3)\ny_tmp = np.array([0,1,0,1,0])\nw_tmp = np.random.rand(X_tmp.shape[1])\nb_tmp = 0.5\nlambda_tmp = 0.7\ndj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n\nprint(f\"dj_db: {dj_db_tmp}\", )\nprint(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )","metadata":{"execution":{"iopub.status.busy":"2024-09-26T05:06:11.671166Z","iopub.execute_input":"2024-09-26T05:06:11.672132Z","iopub.status.idle":"2024-09-26T05:06:11.801666Z","shell.execute_reply.started":"2024-09-26T05:06:11.672082Z","shell.execute_reply":"2024-09-26T05:06:11.800152Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m b_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      6\u001b[0m lambda_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n\u001b[0;32m----> 7\u001b[0m dj_db_tmp, dj_dw_tmp \u001b[38;5;241m=\u001b[39m  \u001b[43mcompute_gradient_logistic_reg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_tmp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdj_db: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdj_db_tmp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegularized dj_dw:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdj_dw_tmp\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, )\n","Cell \u001b[0;32mIn[9], line 20\u001b[0m, in \u001b[0;36mcompute_gradient_logistic_reg\u001b[0;34m(X, y, w, b, lambda_)\u001b[0m\n\u001b[1;32m     17\u001b[0m dj_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m                                       \u001b[38;5;66;03m#scalar\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m):\n\u001b[0;32m---> 20\u001b[0m     f_wb_i \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid\u001b[49m(np\u001b[38;5;241m.\u001b[39mdot(X[i],w) \u001b[38;5;241m+\u001b[39m b)          \u001b[38;5;66;03m#(n,)(n,)=scalar\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     err_i  \u001b[38;5;241m=\u001b[39m f_wb_i  \u001b[38;5;241m-\u001b[39m y[i]                       \u001b[38;5;66;03m#scalar\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n","\u001b[0;31mNameError\u001b[0m: name 'sigmoid' is not defined"],"ename":"NameError","evalue":"name 'sigmoid' is not defined","output_type":"error"}]},{"cell_type":"code","source":"plt.close(\"all\")\ndisplay(output)\nofit = overfit_example(True)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T05:06:12.712941Z","iopub.execute_input":"2024-09-26T05:06:12.714283Z","iopub.status.idle":"2024-09-26T05:06:12.745026Z","shell.execute_reply.started":"2024-09-26T05:06:12.714219Z","shell.execute_reply":"2024-09-26T05:06:12.743258Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m display(\u001b[43moutput\u001b[49m)\n\u001b[1;32m      3\u001b[0m ofit \u001b[38;5;241m=\u001b[39m overfit_example(\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"],"ename":"NameError","evalue":"name 'output' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}