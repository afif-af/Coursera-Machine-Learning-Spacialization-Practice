{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/afifaf20/courserasupervisedweek3lab3?scriptVersionId=198317967\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom lab_utils_common import plot_data, sigmoid, draw_vthresh\nplt.style.use('./deeplearning.mplstyle')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-26T04:33:30.128256Z","iopub.execute_input":"2024-09-26T04:33:30.129083Z","iopub.status.idle":"2024-09-26T04:33:30.531358Z","shell.execute_reply.started":"2024-09-26T04:33:30.12903Z","shell.execute_reply":"2024-09-26T04:33:30.529852Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidget\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlab_utils_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_data, sigmoid, draw_vthresh\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./deeplearning.mplstyle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab_utils_common'"],"ename":"ModuleNotFoundError","evalue":"No module named 'lab_utils_common'","output_type":"error"}]},{"cell_type":"markdown","source":"## Dataset\n\nLet's suppose you have following training dataset\n- The input variable `X` is a numpy array which has 6 training examples, each with two features\n- The output variable `y` is also a numpy array with 6 examples, and `y` is either `0` or `1`","metadata":{}},{"cell_type":"code","source":"X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny = np.array([0, 0, 0, 1, 1, 1]).reshape(-1,1) ","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(1,1,figsize=(4,4))\nplot_data(X, y, ax)\n\nax.axis([0, 4, 0, 3.5])\nax.set_ylabel('$x_1$')\nax.set_xlabel('$x_0$')\nplt.show()","metadata":{"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Canvas(toolbar=None)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34965e0bd92644e1b15e79bcd619e090"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fig,ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplot_data\u001b[49m(X, y, ax)\n\u001b[1;32m      4\u001b[0m ax\u001b[38;5;241m.\u001b[39maxis([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3.5\u001b[39m])\n\u001b[1;32m      5\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$x_1$\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'plot_data' is not defined"],"ename":"NameError","evalue":"name 'plot_data' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"## Logistic regression model\n\n\n* Suppose you'd like to train a logistic regression model on this data which has the form   \n\n  $f(x) = g(w_0x_0+w_1x_1 + b)$\n  \n  where $g(z) = \\frac{1}{1+e^{-z}}$, which is the sigmoid function\n\n\n* Let's say that you trained the model and get the parameters as $b = -3, w_0 = 1, w_1 = 1$. That is,\n\n  $f(x) = g(x_0+x_1-3)$\n\n  (You'll learn how to fit these parameters to the data further in the course)\n  \n  \nLet's try to understand what this trained model is predicting by plotting its decision boundary","metadata":{}},{"cell_type":"markdown","source":"### Refresher on logistic regression and decision boundary\n\n* Recall that for logistic regression, the model is represented as \n\n  $$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) \\tag{1}$$\n\n  where $g(z)$ is known as the sigmoid function and it maps all input values to values between 0 and 1:\n\n  $g(z) = \\frac{1}{1+e^{-z}}\\tag{2}$\n  and $\\mathbf{w} \\cdot \\mathbf{x}$ is the vector dot product:\n  \n  $$\\mathbf{w} \\cdot \\mathbf{x} = w_0 x_0 + w_1 x_1$$\n  \n  \n * We interpret the output of the model ($f_{\\mathbf{w},b}(x)$) as the probability that $y=1$ given $\\mathbf{x}$ and parameterized by $\\mathbf{w}$ and $b$.\n* Therefore, to get a final prediction ($y=0$ or $y=1$) from the logistic regression model, we can use the following heuristic -\n\n  if $f_{\\mathbf{w},b}(x) >= 0.5$, predict $y=1$\n  \n  if $f_{\\mathbf{w},b}(x) < 0.5$, predict $y=0$\n  \n  \n* Let's plot the sigmoid function to see where $g(z) >= 0.5$","metadata":{}},{"cell_type":"code","source":"# Plot sigmoid(z) over a range of values from -10 to 10\nz = np.arange(-10,11)\n\nfig,ax = plt.subplots(1,1,figsize=(5,3))\n# Plot z vs sigmoid(z)\nax.plot(z, sigmoid(z), c=\"b\")\n\nax.set_title(\"Sigmoid function\")\nax.set_ylabel('sigmoid(z)')\nax.set_xlabel('z')\ndraw_vthresh(ax,0)","metadata":{"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Canvas(toolbar=None)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75711b9865c04dbc98c701c22094f64c"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m fig,ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Plot z vs sigmoid(z)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(z, \u001b[43msigmoid\u001b[49m(z), c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSigmoid function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid(z)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'sigmoid' is not defined"],"ename":"NameError","evalue":"name 'sigmoid' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"* As you can see, $g(z) >= 0.5$ for $z >=0$\n\n* For a logistic regression model, $z = \\mathbf{w} \\cdot \\mathbf{x} + b$. Therefore,\n\n  if $\\mathbf{w} \\cdot \\mathbf{x} + b >= 0$, the model predicts $y=1$\n  \n  if $\\mathbf{w} \\cdot \\mathbf{x} + b < 0$, the model predicts $y=0$\n  \n  \n  \n### Plotting decision boundary\n\nNow, let's go back to our example to understand how the logistic regression model is making predictions.\n\n* Our logistic regression model has the form\n\n  $f(\\mathbf{x}) = g(-3 + x_0+x_1)$\n\n\n* From what you've learnt above, you can see that this model predicts $y=1$ if $-3 + x_0+x_1 >= 0$\n\nLet's see what this looks like graphically. We'll start by plotting $-3 + x_0+x_1 = 0$, which is equivalent to $x_1 = 3 - x_0$.\n","metadata":{}},{"cell_type":"code","source":"# Choose values between 0 and 6\nx0 = np.arange(0,6)\n\nx1 = 3 - x0\nfig,ax = plt.subplots(1,1,figsize=(5,4))\n# Plot the decision boundary\nax.plot(x0,x1, c=\"b\")\nax.axis([0, 4, 0, 3.5])\n\n# Fill the region below the line\nax.fill_between(x0,x1, alpha=0.2)\n\n# Plot the original data\nplot_data(X,y,ax)\nax.set_ylabel(r'$x_1$')\nax.set_xlabel(r'$x_0$')\nplt.show()","metadata":{"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Canvas(toolbar=None)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"776921ecb1494ddd8b4ff156f0a2b562"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m ax\u001b[38;5;241m.\u001b[39mfill_between(x0,x1, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Plot the original data\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mplot_data\u001b[49m(X,y,ax)\n\u001b[1;32m     15\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$x_1$\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$x_0$\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'plot_data' is not defined"],"ename":"NameError","evalue":"name 'plot_data' is not defined","output_type":"error"}]},{"cell_type":"code","source":"* In the plot above, the blue line represents the line $x_0 + x_1 - 3 = 0$ and it should intersect the x1 axis at 3 (if we set $x_1$ = 3, $x_0$ = 0) and the x0 axis at 3 (if we set $x_1$ = 0, $x_0$ = 3). \n\n\n* The shaded region represents $-3 + x_0+x_1 < 0$. The region above the line is $-3 + x_0+x_1 > 0$.\n\n\n* Any point in the shaded region (under the line) is classified as $y=0$.  Any point on or above the line is classified as $y=1$. This line is known as the \"decision boundary\".\n\nAs we've seen in the lectures, by using higher order polynomial terms (eg: $f(x) = g( x_0^2 + x_1 -1)$, we can come up with more complex non-linear boundaries.","metadata":{"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[6], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    As we've seen in the lectures, by using higher order polynomial terms (eg: $f(x) = g( x_0^2 + x_1 -1)$, we can come up with more complex non-linear boundaries.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 9)\n"],"ename":"SyntaxError","evalue":"unterminated string literal (detected at line 9) (944506325.py, line 9)","output_type":"error"}]}]}